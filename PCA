In this lab, we perform PCA on the USArrests data set, which is part of the base R package. The rows of the data set contain the 50 states, in alphabetical order.
> states=row.names(USArrests ) > states
The columns of the data set contain the four variables.
> names(USArrests ) [1] "Murder" "Assault " "UrbanPop " "Rape"
We ﬁrst brieﬂy examine the data. We notice that the variables have vastly diﬀerent means.
> apply(USArrests , 2, mean) Murder Assault UrbanPop Rape 7.79 170.76 65.54 21.23
Note that the apply() function allows us to apply a function—in this case, the mean() function—to each row or column of the data set. The second input here denotes whether we wish to compute the mean of the rows, 1, or the columns, 2. We see that there are on average three times as many rapes as murders, and more than eight times as many assaults as rapes. We can also examine the variances of the four variables using the apply() function.
> apply(USArrests , 2, var) Murder Assault UrbanPop Rape 19.0 6945.2 209.5 87.7
402 10. Unsupervised Learning
Not surprisingly, the variables also have vastly diﬀerent variances: the UrbanPop variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of rapes in each state per 100,000 individuals. If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the Assault variable, since it has by far the largest mean and variance. Thus, it is important to standardize the variables to have mean zero and standard deviation one before performing PCA. We now perform principal components analysis using the prcomp() func
prcomp()
tion, which is one of several functions in R that perform PCA.
> pr.out=prcomp(USArrests , scale=TRUE)
By default, the prcomp() function centers the variables to have mean zero. By using the option scale=TRUE, we scale the variables to have standard deviation one. The output from prcomp() contains a number of useful quantities.
> names(pr.out) [1] "sdev" "rotation " "center" "scale" "x"
The center and scale components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.
> pr.out$center Murder Assault UrbanPop Rape 7.79 170.76 65.54 21.23 > pr.out$scale Murder Assault UrbanPop Rape 4.36 83.34 14.47 9.37
The rotation matrix provides the principal component loadings; each column of pr.out$rotation contains the corresponding principal component loading vector.2
> pr.out$rotation PC1 PC2 PC3 PC4 Murder -0.536 0.418 -0.341 0.649 Assault -0.583 0.188 -0.268 -0.743 UrbanPop -0.278 -0.873 -0.378 0.134 Rape -0.543 -0.167 0.818 0.089
We see that there are four distinct principal components. This is to be expected because there are in general min(n−1,p) informative principal components in a data set with n observations and p variables.
2This function names it the rotation matrix, because when we matrix-multiply the X matrix by pr.out$rotation, it gives us the coordinates of the data in the rotated coordinate system. These coordinates are the principal component scores.
10.4 Lab 1: Principal Components Analysis 403
Using the prcomp() function, we do not need to explicitly multiply the data by the principal component loading vectors in order to obtain the principal component score vectors. Rather the 50×4 matrixx has as its columns the principal component score vectors. That is, the kth column is the kth principal component score vector.
> dim(pr.out$x) [1] 50 4
We can plot the ﬁrst two principal components as follows:
> biplot(pr.out , scale=0)
The scale=0 argument to biplot() ensures that the arrows are scaled to
biplot()
represent the loadings; other values for scale give slightly diﬀerent biplots with diﬀerent interpretations. Notice that this ﬁgure is a mirror image of Figure 10.1. Recall that the principal components are only unique up to a sign change, so we can reproduce Figure 10.1 by making a few small changes:
> pr.out$rotation=-pr.out$rotation > pr.out$x=-pr.out$x > biplot(pr.out , scale=0)
The prcomp() function also outputs the standard deviation of each principal component. For instance, on the USArrests data set, we can access these standard deviations as follows:
> pr.out$sdev [1] 1.575 0.995 0.597 0.416
The variance explained by each principal component is obtained by squaring these:
> pr.var=pr.out$sdev ^2 > pr.var [1] 2.480 0.990 0.357 0.173
To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components:
> pve=pr.var/sum(pr.var) > pve [1] 0.6201 0.2474 0.0891 0.0434
We see that the ﬁrst principal component explains 62.0% of the variance in the data, the next principal component explains 24.7% of the variance, and so forth. We can plot the PVE explained by each component, as well as the cumulative PVE, as follows:
> plot(pve , xlab="Principal Component ", ylab="Proportion of Variance Explained ", ylim=c(0,1),type=’b’) > plot(cumsum(pve), xlab="Principal Component ", ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type=’b’)
404 10. Unsupervised Learning
The result is shown in Figure 10.4. Note that the function cumsum() com
cumsum()
putes the cumulative sum of the elements of a numeric vector. For instance:
> a=c(1,2,8,-3) > cumsum(a) [1] 1 3 11 8
